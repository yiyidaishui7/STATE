import os
import glob
import time
import shutil
import subprocess
import pandas as pd
import numpy as np
import scanpy as sc
import matplotlib
matplotlib.use('Agg') # æœåŠ¡å™¨æ— å¤´æ¨¡å¼
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import pearsonr

# ==================== 1. åŸºç¡€é…ç½® ====================

DATA_ROOT = "competition_support_set" 
OUTPUT_ROOT = "competition_40000_500"
METRICS_FILE = f"{OUTPUT_ROOT}/final_score_log.csv"
PLOT_DIR = f"{OUTPUT_ROOT}/plots"
TRAIN_LOG_FILE = f"{OUTPUT_ROOT}/training.log"

# ===  å®˜æ–¹baselineå€¼ (User Provided) ===
BASELINE_DES = 0.106
BASELINE_PDS = 0.514
BASELINE_MAE = 0.027

# ==================== 2. ç¯å¢ƒå‡†å¤‡ ====================

if not os.path.exists(DATA_ROOT):
    # è‡ªåŠ¨ä¸‹è½½æ•°æ®çš„é€»è¾‘å¯ä»¥ä¿ç•™ï¼Œä»¥é˜²ä¸‡ä¸€
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ•°æ®ç›®å½•: {DATA_ROOT}")

# å¼ºåˆ¶ç”Ÿæˆé…ç½®æ–‡ä»¶
CONFIG_PATH = os.path.join(DATA_ROOT, "server_config_2.toml")
with open(CONFIG_PATH, "w") as f:
    f.write(f"""
# Dataset paths - maps dataset names to their directories
[datasets]
replogle_h1 = "competition_support_set/{{hepg2,rpe1,jurkat,k562}}.h5"

# Training specifications
# All cell types in a dataset automatically go into training (excluding zeroshot/fewshot overrides)
[training]
replogle_h1 = "train"

# Zeroshot specifications - entire cell types go to val or test
[zeroshot]
"replogle_h1.hepg2" = "val"


# Fewshot specifications - explicit perturbation lists
[fewshot]
""")

TRAIN_CMD = [
    "uv", "run", "state", "tx", "train",
    f"data.kwargs.toml_config_path={CONFIG_PATH}",
    "data.kwargs.num_workers=8",
    "data.kwargs.batch_col=batch_var",
    "data.kwargs.pert_col=target_gene",
    "data.kwargs.cell_type_key=cell_type",
    "data.kwargs.control_pert=non-targeting",
    f"data.kwargs.perturbation_features_file={DATA_ROOT}/ESM2_pert_features.pt",
    "training.max_steps=40000",
    "training.ckpt_every_n_steps=500",
    "training.val_freq=500",
    "model=state_sm",
    "+wandb.mode=offline",
    f"output_dir={OUTPUT_ROOT}",
    "name=run_3090"
]
CHECKPOINT_DIR = f"{OUTPUT_ROOT}/run_3090/checkpoints"
# VAL_DATA_PATH = f"{DATA_ROOT}/hepg2_val_pure.h5ad"
VAL_DATA_PATH = f"{DATA_ROOT}/hepg2_val_with_controls.h5ad"

TRAIN_SUBSET_PATH = f"{DATA_ROOT}/train_subset_monitor_mixed.h5ad"

# ==================== 3. æ ¸å¿ƒè®¡ç®—é€»è¾‘ ====================

# def ensure_train_subset():
#     if os.path.exists(TRAIN_SUBSET_PATH): return
#     print("â³ ç”Ÿæˆè®­ç»ƒé›†é‡‡æ ·...")
#     try:
#         src = os.path.join(DATA_ROOT, "competition_train.h5")
#         if not os.path.exists(src): src = os.path.join(DATA_ROOT, "k562_gwps.h5")
#         adata = sc.read_h5ad(src)
#         if adata.n_obs > 2000:
#             idx = np.random.choice(adata.n_obs, 2000, replace=False)
#             adata = adata[idx].copy()
#         adata.var_names_make_unique()
#         adata.write(TRAIN_SUBSET_PATH)
#     except Exception as e:
#         print(f"âš ï¸ é‡‡æ ·å¤±è´¥: {e}")


def calculate_competition_scores(pred_path, true_path):
    """
    è®¡ç®— Raw Metrics å¹¶è½¬æ¢ä¸º Scaled Scores å’Œ Overall S
    """
    try:
        adata_pred = sc.read_h5ad(pred_path)
        adata_true = sc.read_h5ad(true_path)
                
        # ==========================================
        # ğŸ›‘ [å…³é”®ä¿®æ”¹] ç®—åˆ†å‰å‰”é™¤ Control (non-targeting)
        # ==========================================
        # æˆ‘ä»¬åªå…³å¿ƒæ¨¡å‹å¯¹"å¾®æ‰°"çš„é¢„æµ‹èƒ½åŠ›ï¼Œä¸å…³å¿ƒå®ƒå¤ç° Control çš„èƒ½åŠ›
        if 'target_gene' in adata_true.obs:
            mask = adata_true.obs['target_gene'] != 'non-targeting'
            
            # å¦‚æœå‰”é™¤åæ²¡å‰©å•¥äº†ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
            if mask.sum() == 0:
                return None
            
            adata_true = adata_true[mask]
            adata_pred = adata_pred[mask] # é¢„æµ‹ç»“æœä¹Ÿè¦å¯¹åº”è¿‡æ»¤
        # ==========================================
        
        common_genes = np.intersect1d(adata_pred.var_names, adata_true.var_names)
        if len(common_genes) == 0: return None
        
        X_pred = adata_pred[:, common_genes].X
        X_true = adata_true[:, common_genes].X
        
        if hasattr(X_pred, "toarray"): X_pred = X_pred.toarray()
        if hasattr(X_true, "toarray"): X_true = X_true.toarray()
        
        # --- 1. è®¡ç®— Raw Metrics ---
        # MAE
        raw_mae = mean_absolute_error(X_true, X_pred)
        
        # DES Proxy (R2)
        # æ³¨æ„ï¼šå®˜æ–¹DESæ˜¯åŸºå› é›†åˆé‡å åº¦ï¼ŒR2æ˜¯å…¶å¼ºç›¸å…³ä»£ç†æŒ‡æ ‡
        raw_des = r2_score(X_true.flatten(), X_pred.flatten())
        
        # PDS Proxy (Pearson Correlation)
        # æ³¨æ„ï¼šå®˜æ–¹PDSæ˜¯åŸºäºL1è·ç¦»çš„æ’åï¼ŒCorrelationæ˜¯å…¶å¼ºç›¸å…³ä»£ç†æŒ‡æ ‡
        n_samples = min(500, X_pred.shape[0])
        idx = np.random.choice(X_pred.shape[0], n_samples, replace=False)
        corrs = []
        for i in idx:
            if np.std(X_pred[i]) == 0 or np.std(X_true[i]) == 0: corrs.append(0)
            else: corrs.append(pearsonr(X_pred[i], X_true[i])[0])
        raw_pds = np.nanmean(corrs)
        
        # --- 2. è®¡ç®— Scaled Scores (å®˜æ–¹å…¬å¼) ---
        
        # DES Scaled: (Your Model DES â€“ Baseline) / (1 â€“ Baseline)
        des_scaled = (raw_des - BASELINE_DES) / (1 - BASELINE_DES)
        des_scaled = max(0, des_scaled) # Negative clipped to 0

        # PDS Scaled: (Your Model PDS â€“ Baseline) / (1 â€“ Baseline)
        pds_scaled = (raw_pds - BASELINE_PDS) / (1 - BASELINE_PDS)
        pds_scaled = max(0, pds_scaled)

        # MAE Scaled: (Baseline - Your Model MAE) / Baseline
        mae_scaled = (BASELINE_MAE - raw_mae) / BASELINE_MAE
        mae_scaled = max(0, mae_scaled)

        # --- 3. Overall Score ---
        # S = 1/3 (DES_scaled + PDS_scaled + MAE_scaled) * 100
        score_s = (1/3) * (des_scaled + pds_scaled + mae_scaled) * 100
        
        return {
            "Raw_MAE": raw_mae, "Raw_DES": raw_des, "Raw_PDS": raw_pds,
            "Scaled_MAE": mae_scaled, "Scaled_DES": des_scaled, "Scaled_PDS": pds_scaled,
            "Score_S": score_s
        }
    except Exception as e:
        print(f"âŒ è®¡ç®—é”™è¯¯: {e}")
        return None

def run_inference(ckpt, input_adata):
    out_file = f"{OUTPUT_ROOT}/temp_pred_{os.path.basename(ckpt)}.h5ad"
    cmd = [
        "uv", "run", "state", "tx", "infer",
        f"--output={out_file}",
        f"--model-dir={OUTPUT_ROOT}/run_3090",
        f"--checkpoint={ckpt}",
        f"--adata={input_adata}",
        "--pert-col=target_gene"
    ]
    
    # ==========================================
    #  [å…³é”®ä¿®æ”¹] æŒ‡å®šæ¨ç†ä½¿ç”¨ GPU 1
    # ==========================================
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = "3"  # <--- å¼ºåˆ¶ä½¿ç”¨ 1 å·å¡
    
    try:
        # æ³¨æ„è¿™é‡Œå¢åŠ äº† env=env å‚æ•°
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, check=True, env=env)
        return out_file
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return None
        
def save_dashboard(history_df, step):
    """
    æ—  Emoji ç‰ˆï¼šä¿®å¤æœåŠ¡å™¨å­—ä½“ç¼ºå¤±è­¦å‘Š
    """
    if history_df.empty: return
    os.makedirs(PLOT_DIR, exist_ok=True)
    
    # ç¡®ä¿ä½¿ç”¨æ— å¤´æ¨¡å¼
    plt.switch_backend('Agg') 
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 14))
    steps = history_df['step']
    
    # ==========================================
    # å›¾ 1: Overall Score S
    # ==========================================
    ax = axes[0, 0]
    # Val (çº¢å®çº¿)
    if 'val_Score_S' in history_df.columns:
        ax.plot(steps, history_df['val_Score_S'], 'r-o', linewidth=2, label='Val Score (Test)')
        max_s = history_df['val_Score_S'].max()
        ax.axhline(y=max_s, color='red', linestyle=':', alpha=0.3)
        ax.text(steps.iloc[-1], max_s, f" Val Max: {max_s:.2f}", va='center', color='red', fontweight='bold')
    
    # Train (è“è™šçº¿)
    if 'train_Score_S' in history_df.columns:
        ax.plot(steps, history_df['train_Score_S'], 'b--x', linewidth=1.5, alpha=0.6, label='Train Score (Fit)')
        
    ax.set_title("Overall Score: Train vs Val") # ğŸ† å·²åˆ é™¤
    ax.set_ylabel("Score (0-100)")
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)
    
    # ==========================================
    # å›¾ 2: Val Score Components
    # ==========================================
    ax = axes[0, 1]
    if 'val_Scaled_DES' in history_df.columns:
        ax.plot(steps, history_df['val_Scaled_DES'], label='Val DES (R2)', color='tab:blue')
        ax.plot(steps, history_df['val_Scaled_PDS'], label='Val PDS (Corr)', color='tab:orange')
        ax.plot(steps, history_df['val_Scaled_MAE'], label='Val MAE (Error)', color='tab:green', linewidth=2)
    
    ax.set_title("Val Score Composition") # ğŸ“Š å·²åˆ é™¤
    ax.set_ylim(-0.1, 1.1)
    ax.legend()
    ax.grid(True, alpha=0.3)

    # ==========================================
    # å›¾ 3: Raw MAE
    # ==========================================
    ax = axes[1, 0]
    if 'val_Raw_MAE' in history_df.columns:
        ax.plot(steps, history_df['val_Raw_MAE'], 'r-o', label='Val MAE')
    if 'train_Raw_MAE' in history_df.columns:
        ax.plot(steps, history_df['train_Raw_MAE'], 'b--x', alpha=0.6, label='Train MAE')
        
    ax.axhline(y=BASELINE_MAE, color='black', linestyle='--', linewidth=2, label=f'Baseline ({BASELINE_MAE})')
    
    ax.set_title("Raw MAE (Lower is Better)") # ğŸ“‰ å·²åˆ é™¤
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # ==========================================
    # å›¾ 4: Biological Signals
    # ==========================================
    ax = axes[1, 1]
    
    # PDS (Correlation)
    if 'val_Raw_PDS' in history_df.columns:
        ax.plot(steps, history_df['val_Raw_PDS'], color='tab:orange', linestyle='-', label='Val PDS')
    if 'train_Raw_PDS' in history_df.columns:
        ax.plot(steps, history_df['train_Raw_PDS'], color='tab:orange', linestyle='--', alpha=0.5, label='Train PDS')
    ax.axhline(y=BASELINE_PDS, color='tab:orange', linestyle=':', label='Base PDS')

    # DES (R2)
    if 'val_Raw_DES' in history_df.columns:
        ax.plot(steps, history_df['val_Raw_DES'], color='tab:blue', linestyle='-', label='Val DES')
    if 'train_Raw_DES' in history_df.columns:
        ax.plot(steps, history_df['train_Raw_DES'], color='tab:blue', linestyle='--', alpha=0.5, label='Train DES')
    ax.axhline(y=BASELINE_DES, color='tab:blue', linestyle=':', label='Base DES')

    ax.set_title("Biological Signals: Train(dashed) vs Val(solid)") # ğŸ§¬ å·²åˆ é™¤
    ax.legend(ncol=2, fontsize='small')
    ax.grid(True, alpha=0.3)

    plt.suptitle(f"Training Monitor - Step {step}", fontsize=16)
    plt.tight_layout()
    plt.savefig(f"{PLOT_DIR}/latest_score_card.png")
    plt.savefig(f"{PLOT_DIR}/step_{step:05d}.png")
    plt.close()

# ==================== 4. ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print(f"ğŸš€ [Score train-500/val-500] å¯åŠ¨è®­ç»ƒ (Baseline: DES={BASELINE_DES}, PDS={BASELINE_PDS}, MAE={BASELINE_MAE})...")
    
    # === 1. æ¸…ç†æ—§ç›®å½• (ç¡®ä¿æ˜¯ä»å¤´å¼€å§‹) ===
    if os.path.exists(OUTPUT_ROOT):
        try:
            shutil.rmtree(OUTPUT_ROOT)
            print(f"ğŸ§¹ å·²æ¸…ç†æ—§ç›®å½•: {OUTPUT_ROOT}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ç›®å½•å¤±è´¥: {e}")
            
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    
    # âš ï¸ æ³¨é‡Šæ‰æ­¤è¡Œï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨ç”Ÿæˆäº†æ›´å¥½çš„ mixed è®­ç»ƒç›‘æ§é›†
    # ensure_train_subset() 
    
    log_file = open(TRAIN_LOG_FILE, "w")
    print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†å†™å…¥: {TRAIN_LOG_FILE}")
    
    # === 2. å¯åŠ¨è®­ç»ƒè¿›ç¨‹ (å¼ºåˆ¶ä½¿ç”¨ GPU 2) ===
    # è¿™æ ·å¯ä»¥é˜²æ­¢è®­ç»ƒè¿›ç¨‹æŠ¢å  GPU 3 çš„æ˜¾å­˜ï¼Œç•™ç»™æ¨ç†è¿›ç¨‹ä¸“ç”¨
    train_env = os.environ.copy()
    train_env["CUDA_VISIBLE_DEVICES"] = "2" 

    try:
        process = subprocess.Popen(
            TRAIN_CMD, 
            stdout=log_file, 
            stderr=subprocess.STDOUT, 
            text=True, 
            bufsize=1,
            env=train_env # <--- æ³¨å…¥ç¯å¢ƒå˜é‡
        )
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°å‘½ä»¤ (å¦‚ uv/state)ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒã€‚")
        sys.exit(1)

    processed_ckpts = set()
    history_df = pd.DataFrame()

    # === 3. å¯åŠ¨ç¼“å†²æ£€æŸ¥ (é˜²æ­¢åˆšå¯åŠ¨å°±æŒ‚) ===
    print("â³ ç­‰å¾…è®­ç»ƒè¿›ç¨‹å¯åŠ¨ (5ç§’)...")
    time.sleep(5)
    if process.poll() is not None:
        print(f"âŒ è®­ç»ƒè¿›ç¨‹ç«‹å³é€€å‡ºï¼Exit Code: {process.returncode}")
        log_file.close()
        # æ‰“å°æ—¥å¿—å°¾éƒ¨å¸®åŠ©æ’é”™
        try:
            with open(TRAIN_LOG_FILE, 'r') as f:
                print("".join(f.readlines()[-20:]))
        except: pass
        sys.exit(1)
    else:
        print("âœ… è®­ç»ƒè¿›ç¨‹ (GPU 2) è¿è¡Œä¸­... ç›‘æ§è„šæœ¬ (GPU 3) å‡†å¤‡å°±ç»ªã€‚")

    try:
        while True:
            if process.poll() is not None:
                print("ğŸ è®­ç»ƒç»“æŸã€‚")
                break
            
            # 1. è·å–æ‰€æœ‰ ckpt æ–‡ä»¶
            raw_ckpts = glob.glob(f"{CHECKPOINT_DIR}/*.ckpt")
            
            # === âœ¨ å¯»æ‰¾å¹¶æ‰“å°æœ€ä½³æ¨¡å‹ä¿¡æ¯ (ä½ çš„æ–°å¢é€»è¾‘) ===
            best_model_info = "å°šæœªäº§ç”Ÿ"
            best_step_num = -1
            import re
            
            for c in raw_ckpts:
                base = os.path.basename(c)
                if "val_loss" in base:
                    try:
                        best_model_info = base
                        m_best = re.search(r"step=(\d+)", base)
                        if m_best: best_step_num = int(m_best.group(1))
                    except: pass
            
            if best_step_num != -1:
                # ç¨å¾®ç¾åŒ–ä¸€ä¸‹è¾“å‡ºï¼Œé¿å…åˆ·å±å¤ªå¿«
                pass 
                # print(f"ğŸŒŸ [å½“å‰æœ€ä½³] Step: {best_step_num}") 

            # === 2. è¿‡æ»¤ä¸æ’åº ===
            ckpts = []
            for c in raw_ckpts:
                filename = os.path.basename(c)
                
                # ğŸ›‘ è¿‡æ»¤ï¼šè·³è¿‡ val_loss å‰¯æœ¬ å’Œ é‡å¤å‘½åçš„æ–‡ä»¶
                if "val_loss" in filename or "step=step" in filename:
                    continue
                
                m = re.search(r"step=(\d+)", filename)
                if m: 
                    step_num = int(m.group(1))
                    ckpts.append((step_num, c))
            
            ckpts.sort(key=lambda x: x[0])
            
            # === 3. éå†æ‰§è¡Œæ¨ç† ===
            new_data = False
            for step, ckpt_path in ckpts:
                if ckpt_path in processed_ckpts: continue
                
                # åœ¨æ‰“å° Step æ—¶é¡ºä¾¿æ˜¾ç¤ºä¸€ä¸‹å½“å‰çš„æœ€ä½³çŠ¶æ€
                best_info_str = f" (ğŸŒŸ Best: {best_step_num})" if best_step_num != -1 else ""
                print(f"[{time.strftime('%H:%M')}] ğŸ” Step {step}{best_info_str} ...")
                
                row = {'step': step, 'timestamp': time.strftime('%Y-%m-%d %H:%M')}
                
                # Val Set (GPU 1)
                # ç¡®ä¿ calculate_competition_scores å¤„ç† None çš„æƒ…å†µ
                pred_val = run_inference(ckpt_path, VAL_DATA_PATH)
                if pred_val:
                    val_m = calculate_competition_scores(pred_val, VAL_DATA_PATH)
                    if val_m:
                        for k, v in val_m.items(): row[f"val_{k}"] = v
                        print(f"   ğŸ† Score: {val_m['Score_S']:.2f} (MAE_s:{val_m['Scaled_MAE']:.2f})")
                    if os.path.exists(pred_val): os.remove(pred_val)

                # Train Set (GPU 1)
                if os.path.exists(TRAIN_SUBSET_PATH):
                    pred_train = run_inference(ckpt_path, TRAIN_SUBSET_PATH)
                    if pred_train:
                        train_m = calculate_competition_scores(pred_train, TRAIN_SUBSET_PATH)
                        if train_m:
                            for k, v in train_m.items(): row[f"train_{k}"] = v
                        if os.path.exists(pred_train): os.remove(pred_train)
                
                # å†™å…¥ CSV
                df_row = pd.DataFrame([row])
                history_df = pd.concat([history_df, df_row], ignore_index=True)
                history_df.to_csv(METRICS_FILE, index=False)
                
                processed_ckpts.add(ckpt_path)
                new_data = True
            
            if new_data:
                save_dashboard(history_df, history_df['step'].iloc[-1])
            
            time.sleep(60)

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·åœæ­¢è®­ç»ƒã€‚")
        process.terminate()
    finally:
        log_file.close()