import os
import pandas as pd
import numpy as np
import scanpy as sc
import subprocess
import glob
from cell_eval import MetricsEvaluator, score_agg_metrics

# ================= é…ç½®åŒºåŸŸ (è¯·ç¡®è®¤è·¯å¾„) =================
OUTPUT_ROOT = "competition_hepg2_official"
VAL_DATA_PATH = "competition_support_set/hepg2_val_with_controls.h5ad"
DATA_ROOT = "competition_support_set"

# å°è¯•è‡ªåŠ¨å¯»æ‰¾ä¸€ä¸ª step=1000 çš„ ckptï¼Œå¦‚æœæ‰¾ä¸åˆ°è¯·æ‰‹åŠ¨ä¿®æ”¹ä¸‹é¢çš„ path
CKPT_SEARCH = f"{OUTPUT_ROOT}/run_3090/checkpoints/step=1000.ckpt"
found_ckpts = glob.glob(CKPT_SEARCH)
if found_ckpts:
    CHECKPOINT_PATH = found_ckpts[0]
else:
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·åœ¨è¿™é‡Œå¡«å…¥ç»å¯¹è·¯å¾„
    CHECKPOINT_PATH = "ä½ çš„ckptæ–‡ä»¶çš„ç»å¯¹è·¯å¾„.ckpt"

print(f"ğŸ¯ é€‰å®šçš„ Checkpoint: {CHECKPOINT_PATH}")

# å®˜æ–¹ Baseline (å¿…é¡»æ˜¯è¿™ä¸‰åˆ—)
BASELINE_DF = pd.DataFrame([{
    'mae': 0.027,          
    'r2_mean': 0.106,      
    'pearson_mean': 0.514  
}])
# =======================================================

def step1_inference():
    print(f"\nğŸ” [1/3] å¼€å§‹æ¨ç†...")
    out_file = "debug_pred.h5ad"
    
    # å¦‚æœå·²ç»æ¨ç†è¿‡ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡ä»¥èŠ‚çœæ—¶é—´ (å¯é€‰)
    # if os.path.exists(out_file):
    #     print("   -> æ£€æµ‹åˆ° debug_pred.h5ad å·²å­˜åœ¨ï¼Œè·³è¿‡æ¨ç†ã€‚")
    #     return out_file

    cmd = [
        "uv", "run", "state", "tx", "infer",
        f"--output={out_file}",
        f"--model-dir={OUTPUT_ROOT}/run_3090",
        f"--checkpoint={CHECKPOINT_PATH}",
        f"--adata={VAL_DATA_PATH}",
        "--pert-col=target_gene"
    ]
    
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = "1"
    
    try:
        subprocess.run(cmd, check=True, env=env)
        print("âœ… æ¨ç†å®Œæˆï¼Œå·²ç”Ÿæˆ debug_pred.h5ad")
        return out_file
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return None

def step2_compute_metrics(pred_path):
    print(f"\nğŸ” [2/3] è®¡ç®— Raw Metrics (MetricsEvaluator)...")
    adata_pred = sc.read_h5ad(pred_path)
    adata_true = sc.read_h5ad(VAL_DATA_PATH)
    
    # åŸºå› å¯¹é½
    common = np.intersect1d(adata_pred.var_names, adata_true.var_names)
    adata_pred = adata_pred[:, common].copy()
    adata_true = adata_true[:, common].copy()
    
    print("   Running compute() (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    evaluator = MetricsEvaluator(
        adata_pred=adata_pred,
        adata_real=adata_true,
        pert_col="target_gene",
        control_pert="non-targeting",
        num_threads=16
    )
    
    _, agg_results = evaluator.compute()
    
    # è½¬æ¢ä¸º DataFrame
    if isinstance(agg_results, dict):
        raw_df = pd.DataFrame([agg_results])
    else:
        raw_df = agg_results

    return raw_df

def step3_clean_and_score(raw_df):
    print(f"\nğŸ” [3/3] æ•°æ®æ¸…æ´—ä¸æ‰“åˆ† (Fix Shapes)...")

    # ==========================================
    # ğŸ›‘ [å…³é”®ä¿®å¤] Polars -> Pandas è½¬æ¢
    # ==========================================
    # æŠ¥é”™æ˜¯å› ä¸º raw_df æ˜¯ polars å¯¹è±¡ï¼Œä¸æ”¯æŒ Pandas çš„ç­›é€‰è¯­æ³•
    try:
        if hasattr(raw_df, "to_pandas"):
            print("   -> æ£€æµ‹åˆ° Polars DataFrameï¼Œæ­£åœ¨è½¬æ¢ä¸º Pandas...")
            raw_df = raw_df.to_pandas()
    except Exception as e:
        print(f"âš ï¸ è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¤„ç†: {e}")

    # === 1. æ•°æ®æ¸…æ´— (æå– mean è¡Œ) ===
    # ç°åœ¨å®ƒå·²ç»æ˜¯ Pandas äº†ï¼ŒåŸæ¥çš„ä»£ç å°±èƒ½æ­£å¸¸å·¥ä½œäº†
    if 'statistic' in raw_df.columns:
        mean_row = raw_df[raw_df['statistic'] == 'mean']
        if mean_row.empty:
            print("âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° statistic='mean' çš„è¡Œï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€è¡Œ")
            mean_row = raw_df.iloc[[0]]
    else:
        mean_row = raw_df.iloc[[0]]
    
    # === 2. åˆ—åæ˜ å°„ (Mapping) ===
    # æå–å®˜æ–¹éœ€è¦çš„ 3 ä¸ªæŒ‡æ ‡
    
    # (A) MAE
    val_mae = float(mean_row['mae'].iloc[0])
    
    # (B) Pearson (PDS)
    # ä¼˜å…ˆæ‰¾ 'pearson_mean' (å¦‚æœ cell-eval ç‰ˆæœ¬æ›´æ–°äº†)ï¼Œå¦åˆ™æ‰¾ 'pearson_delta'
    if 'pearson_mean' in mean_row.columns:
        val_pearson = float(mean_row['pearson_mean'].iloc[0])
    elif 'pearson_delta' in mean_row.columns:
        val_pearson = float(mean_row['pearson_delta'].iloc[0])
    else:
        print("âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° Pearson ç›¸å…³åˆ—ï¼Œè®¾ä¸º 0")
        val_pearson = 0.0

    # (C) R2 (DES)
    # ä¼˜å…ˆæ‰¾ 'r2_mean'ï¼Œå¦åˆ™æ‰¾ 'r2'ï¼Œéƒ½æ²¡æœ‰å°±è®¾ä¸º 0 (é˜²æ­¢æŠ¥é”™)
    if 'r2_mean' in mean_row.columns:
        val_r2 = float(mean_row['r2_mean'].iloc[0])
    elif 'r2' in mean_row.columns:
        val_r2 = float(mean_row['r2'].iloc[0])
    else:
        print("âš ï¸ è­¦å‘Š: ç»“æœä¸­æ²¡æœ‰ R2 (DES)ï¼Œæš‚æ—¶è®¾ä¸º 0.0")
        val_r2 = 0.0
        
    # === 3. æ„é€ å¹²å‡€çš„ User DataFrame ===
    user_clean_df = pd.DataFrame([{
        'mae': val_mae,
        'r2_mean': val_r2,
        'pearson_mean': val_pearson
    }])
    
    print("\nğŸ“Š [Cleaned Data for Scoring]")
    print(user_clean_df)
    print("-" * 30)

    # === 4. è°ƒç”¨å®˜æ–¹æ‰“åˆ† ===
    try:
        score_df = score_agg_metrics(
            results_user=user_clean_df,
            results_base=BASELINE_DF,
            output="debug_final_score.csv"
        )
        
        print("\nâœ… [Success] æ‰“åˆ†æˆåŠŸï¼ç»“æœå¦‚ä¸‹ï¼š")
        if score_df is None:
            # æœ‰äº›ç‰ˆæœ¬åªå†™æ–‡ä»¶ä¸è¿”å›ï¼Œæ‰‹åŠ¨è¯»å–
            if os.path.exists("debug_final_score.csv"):
                score_df = pd.read_csv("debug_final_score.csv")
            else:
                print("âŒ æœªç”Ÿæˆåˆ†æ•°æ–‡ä»¶")
                return

        print(score_df)
        
        # æå–æœ€ç»ˆåˆ†
        final_s = 0
        if 'total_score' in score_df.columns:
            final_s = score_df['total_score'].iloc[0] * 100
        elif 'score' in score_df.columns:
            final_s = score_df['score'].iloc[0] * 100
            
        print(f"\nğŸ† æœ€ç»ˆå¾—åˆ† (Score S): {final_s:.2f}")
        
    except Exception as e:
        print(f"âŒ æ‰“åˆ†ä¾ç„¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
if __name__ == "__main__":
    # 1. æ¨ç†
    pred_file = step1_inference()
    if pred_file:
        # 2. è®¡ç®—åŸå§‹æŒ‡æ ‡
        raw_metrics = step2_compute_metrics(pred_file)
        # 3. æ¸…æ´—å¹¶æ‰“åˆ†
        step3_clean_and_score(raw_metrics)